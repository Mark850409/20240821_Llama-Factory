{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RomxmJkjnqE",
        "outputId": "c9e61fe5-50e8-44d5-d535-23e71613e477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 21 02:44:28 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-XmSvFj1gg"
      },
      "source": [
        "#STEP 0 確認有使用GPU\n",
        "##記事本必須在GPU模式下運行，可以執行上方的nvidia-smi查看GPU是否有GPU。若没有，點擊左上角的 編輯>筆記本設定，把硬體加速器改成GPU。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2PYE5UE4cCc",
        "outputId": "23166e56-b471-400a-9586-5f2b0e73b00f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 16242, done.\u001b[K\n",
            "remote: Counting objects: 100% (6978/6978), done.\u001b[K\n",
            "remote: Compressing objects: 100% (425/425), done.\u001b[K\n",
            "remote: Total 16242 (delta 6627), reused 6611 (delta 6550), pack-reused 9264 (from 1)\u001b[K\n",
            "Receiving objects: 100% (16242/16242), 223.01 MiB | 14.35 MiB/s, done.\n",
            "Resolving deltas: 100% (12077/12077), done.\n",
            "/content/LLaMA-Factory\n",
            "Requirement already satisfied: transformers<=4.43.4,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.42.4)\n",
            "Collecting datasets<=2.20.0,>=2.16.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate<=0.32.0,>=0.30.1 (from -r requirements.txt (line 3))\n",
            "  Downloading accelerate-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from -r requirements.txt (line 6))\n",
            "  Downloading gradio-4.41.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
            "Collecting tiktoken (from -r requirements.txt (line 11))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.20.3)\n",
            "Collecting uvicorn (from -r requirements.txt (line 13))\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.8.2)\n",
            "Collecting fastapi (from -r requirements.txt (line 15))\n",
            "  Downloading fastapi-0.112.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from -r requirements.txt (line 16))\n",
            "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.7.1)\n",
            "Collecting fire (from -r requirements.txt (line 18))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (0.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (3.10.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (2.3.1+cu121)\n",
            "Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n",
            "  Downloading tyro-0.8.8-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading ruff-0.6.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.0.7)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 13))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 14)) (2.20.1)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->-r requirements.txt (line 15))\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 18)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<=4.43.4,>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5)) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate<=0.32.0,>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.32.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.41.0-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading tyro-0.8.8-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m182.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=7220be000cfe48d6cc6fffc3906d43ab68928b1d3f815d37bb4ccd695530918d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: pydub, xxhash, websockets, tomlkit, shtab, semantic-version, ruff, python-multipart, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, h11, fsspec, fire, ffmpy, dill, aiofiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, fastapi, gradio-client, datasets, gradio, accelerate, trl, peft\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.0 aiofiles-23.2.1 datasets-2.20.0 dill-0.3.8 fastapi-0.112.1 ffmpy-0.4.0 fire-0.6.0 fsspec-2024.5.0 gradio-4.41.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 orjson-3.10.7 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.1 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 starlette-0.38.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.8 uvicorn-0.30.6 websockets-12.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 1 複製儲存庫並安裝必要的函式庫\n",
        "#@markdown #STEP 1\n",
        "#@markdown ##複製儲存庫並安裝必要的函式庫\n",
        "#@markdown ##Clone repository & Install requirements lib\n",
        "\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd ./LLaMA-Factory\n",
        "!pip install -r requirements.txt\n",
        "!pip install bitsandbytes>=0.39.0\n",
        "\n",
        "#用比較殘暴的方式開啟 Gradio 的分享連結\n",
        "train_web_py_file_path = \"./src/webui.py\"\n",
        "try:\n",
        "  with open(train_web_py_file_path, 'r') as file:\n",
        "    file_content = file.read()\n",
        "  modified_content = file_content.replace(\"share=False\", \"share=True\")\n",
        "  with open(train_web_py_file_path, 'w') as file:\n",
        "    file.write(modified_content)\n",
        "except Exception as e:\n",
        "    print(f'ERROR: {str(e)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title STEP 1-2 安裝pyngrok {\"display-mode\":\"both\"}\n",
        "# @markdown #STEP 1-2\n",
        "# @markdown ## 此工具為內網穿透工具，才可能將web server暴露在公開位置\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "s26wnzkbgKsf",
        "outputId": "8f1b4285-0c56-4485-c4e1-d15e358fd4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title STEP 1-3 開啟7860 ports {\"display-mode\":\"both\"}\n",
        "# @markdown #STEP 1-3\n",
        "# @markdown  ## 指定要內網穿透的ports\n",
        "# @markdown\n",
        "# @markdown ## 開啟前會清除所有已執行的隧道，且設定token\n",
        "from pyngrok import ngrok\n",
        "# 停止所有隧道\n",
        "ngrok.kill()\n",
        "\n",
        "# 替換下面的 YOUR_AUTHTOKEN 為你從 ngrok 獲取的 Authtoken\n",
        "ngrok.set_auth_token(\"2Wbru4ocvqZEdhYqSB6Zq6OQwno_2atqQkJP8Y9bRVPin9H1A\")\n",
        "\n",
        "# 開啟端口 7860 的 ngrok 隧道\n",
        "public_url = ngrok.connect(7860)\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "u9NrhcllgL_I",
        "outputId": "2f83e788-f5c9-4627-d93c-6a766c5d11f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://ce43-35-197-138-13.ngrok-free.app\" -> \"http://localhost:7860\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title STEP 1-4 安裝llama-factory必要指令\n",
        "# @markdown #STEP 1-4\n",
        "# @markdown ## 請注意，這步驟一定要安裝，否則在訓練模型時會出錯\n",
        "pip install -e .[torch,metrics]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5VHW4PE6REh",
        "outputId": "dda6f871-0cd0-4a11-9dfe-b77d72cad1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.43.4,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.42.4)\n",
            "Requirement already satisfied: datasets<=2.20.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.20.0)\n",
            "Requirement already satisfied: accelerate<=0.32.0,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.32.0)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.41.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.1.99)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.30.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.112.1)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.8.4.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.10.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (6.4.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.10.7)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.6.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.0.7)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.4.dev0) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->llamafactory==0.8.4.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->llamafactory==0.8.4.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0) (0.8.8)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.4.dev0) (0.38.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.8.4.dev0) (1.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.1.2)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=22083 sha256=7882c1da237058ee5e3976c59d8d2ecb545ff47b202a910c34a5637008d40396\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6zrft10q/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: rouge-chinese, llamafactory\n",
            "Successfully installed llamafactory-0.8.4.dev0 rouge-chinese-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiFJX0-d6thC",
        "outputId": "29689c05-1510-408a-d967-f2fe67c5bf05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-21 03:10:44.807489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-21 03:10:44.827142: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-21 03:10:44.833611: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-21 03:10:44.848108: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-21 03:10:46.061571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 763, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 288, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1931, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1516, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 826, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/common.py\", line 134, in get_model_info\n",
            "    return get_model_path(model_name), get_template(model_name), get_visual(model_name)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/common.py\", line 107, in get_model_path\n",
            "    model_path = user_config[\"path_dict\"].get(model_name, \"\") or path_dict.get(DownloadSource.DEFAULT, \"\")\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "2024-08-21 03:12:16.372561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-21 03:12:16.392563: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-21 03:12:16.400958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-21 03:12:17.960821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/21/2024 03:12:24 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "08/21/2024 03:12:24 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 100% 51.3k/51.3k [00:00<00:00, 223kB/s]\n",
            "tokenizer.json: 100% 9.08M/9.08M [00:00<00:00, 19.6MB/s]\n",
            "special_tokens_map.json: 100% 97.0/97.0 [00:00<00:00, 652kB/s]\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 03:12:27,455 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 03:12:27,456 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 03:12:27,456 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 03:12:27,456 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-08-21 03:12:27,839 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "08/21/2024 03:12:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "08/21/2024 03:12:27 - INFO - llamafactory.data.loader - Loading dataset mistral_dataset.json...\n",
            "Generating train split: 78 examples [00:00, 1465.86 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 78/78 [00:00<00:00, 145.96 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 78/78 [00:13<00:00,  5.90 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 9125, 128007, 271, 57668, 112599, 107226, 25129, 104150, 114775, 44510, 238, 105469, 9554, 106499, 108327, 104241, 104599, 3922, 101658, 223, 19361, 104214, 95598, 116516, 50338, 104241, 104150, 114775, 34208, 102987, 112950, 9554, 74245, 121871, 113312, 105085, 48634, 1811, 109311, 120248, 104398, 109579, 106057, 21601, 9554, 45309, 109778, 242, 81742, 120793, 120610, 1041, 102159, 126644, 104241, 103214, 10110, 47586, 6938, 68206, 75376, 220, 110310, 49988, 102781, 21043, 29411, 16, 13, 220, 120248, 104398, 34273, 109, 121103, 74257, 102159, 104241, 103214, 16325, 106499, 108327, 104241, 9554, 127234, 120920, 25340, 226, 25129, 48816, 6938, 68206, 42246, 37656, 25129, 34208, 116900, 25129, 9554, 96412, 104577, 9174, 17, 13, 102008, 251, 104314, 1, 28589, 10919, 48816, 49203, 1, 25340, 226, 25129, 3922, 103768, 11883, 57668, 44510, 238, 105469, 9554, 106499, 108327, 104150, 114775, 34208, 113312, 105085, 48634, 3922, 71461, 116748, 102987, 17701, 50338, 121103, 1811, 103864, 102159, 50338, 121103, 106877, 109638, 29411, 256, 482, 113617, 104087, 53229, 17161, 9554, 21405, 28190, 96412, 104577, 3922, 30775, 244, 19012, 246, 34226, 102987, 114050, 9554, 113312, 91774, 198, 256, 482, 9085, 252, 235, 17701, 57668, 19967, 101399, 103087, 102987, 106499, 108327, 104241, 104599, 9554, 124649, 66378, 91774, 50338, 198, 256, 482, 94983, 17701, 53229, 17161, 39442, 29172, 82317, 78388, 123111, 9554, 50338, 106336, 3922, 44510, 238, 105469, 64531, 104146, 50338, 121103, 198, 256, 482, 123516, 101557, 106, 104241, 123717, 16325, 102208, 105714, 104241, 55030, 56965, 9554, 50021, 106483, 105532, 108826, 34208, 27327, 33857, 89753, 91875, 198, 256, 482, 94785, 84844, 34226, 77913, 104146, 5486, 106340, 116958, 26892, 104760, 110698, 124118, 32018, 64467, 73686, 104123, 198, 18, 13, 85241, 252, 37292, 2617, 5742, 220, 17228, 6726, 20, 111671, 3922, 103963, 33671, 50338, 121103, 57237, 17228, 8504, 15, 3922, 55999, 914, 103214, 104241, 123717, 9174, 19, 13, 121391, 103214, 50338, 121103, 106877, 120610, 29411, 256, 482, 65488, 234, 123717, 104817, 105233, 7, 29245, 119, 31958, 340, 256, 482, 65488, 234, 103214, 103214, 13153, 7, 29245, 119, 31958, 340, 256, 482, 220, 110310, 116718, 50338, 121103, 10110, 120610, 106340, 116958, 26892, 104760, 102789, 50338, 121103, 55999, 103664, 1049, 19113, 11, 55999, 17620, 17, 38574, 50338, 121103, 102789, 26892, 104760, 23954, 20, 13, 74662, 50338, 121103, 39177, 3922, 109311, 78698, 101557, 106, 88852, 120351, 106341, 29411, 256, 482, 65488, 234, 123717, 16325, 102208, 105714, 104241, 9554, 82912, 34208, 50021, 106483, 105532, 108826, 198, 256, 482, 108519, 116900, 25129, 9554, 117375, 198, 256, 482, 49854, 99, 116958, 26892, 104760, 7, 102789, 104241, 28190, 50021, 60358, 31634, 19361, 114640, 103051, 113849, 9554, 26892, 104760, 8, 58291, 123120, 64467, 59563, 198, 21, 13, 88435, 125672, 102321, 119318, 78657, 17297, 11, 109311, 103963, 102867, 33091, 69905, 115201, 20834, 112745, 198, 104241, 123717, 104817, 105233, 25, 220, 17228, 931, 16, 198, 104241, 103214, 103214, 13153, 25, 105382, 232, 14276, 244, 102397, 320, 37656, 25129, 121782, 115023, 8239, 235, 102397, 320, 116900, 25129, 121782, 115023, 8239, 235, 101538, 34547, 320, 37656, 25129, 340, 116718, 50338, 121103, 29411, 112561, 14276, 244, 102397, 9554, 37656, 25129, 120220, 20379, 57668, 113230, 9554, 125641, 121769, 107328, 105843, 120360, 35287, 119702, 103786, 37656, 115070, 110757, 33208, 122190, 102789, 115023, 8239, 235, 102397, 116900, 25129, 9554, 13393, 233, 54322, 64063, 102057, 249, 50021, 105324, 106877, 3922, 109163, 20379, 102700, 57668, 106310, 97655, 58291, 92776, 106812, 110039, 15120, 38574, 110482, 103496, 16325, 50338, 101171, 104, 1811, 103864, 109757, 105714, 104241, 119046, 52563, 57668, 106310, 9554, 105532, 108826, 106812, 68171, 64063, 102057, 249, 16325, 50338, 54322, 3922, 103130, 69636, 103130, 17701, 34226, 111764, 5486, 87447, 31958, 9554, 106090, 38574, 1811, 115023, 8239, 235, 101538, 121964, 22649, 105487, 102789, 113312, 105085, 48634, 106812, 19000, 103864, 103188, 39607, 16325, 72718, 28037, 105532, 112258, 113, 113266, 119101, 57668, 106310, 27327, 19000, 40474, 99750, 17905, 124778, 13153, 109809, 34208, 22649, 34171, 9174, 26892, 104760, 102789, 64467, 73686, 29411, 26892, 104760, 57668, 106310, 107740, 103864, 38574, 120273, 101513, 71461, 117859, 103984, 103031, 33091, 9554, 119745, 102789, 30832, 105888, 113415, 23897, 87447, 54322, 9554, 107328, 27479, 109169, 30177, 116879, 111671, 1811, 106340, 104983, 17905, 3922, 112561, 14276, 244, 102397, 109163, 20379, 120523, 58291, 110757, 33208, 121591, 110008, 75863, 74770, 78698, 101557, 106, 102993, 15120, 38574, 107099, 23538, 3922, 114816, 115076, 24273, 19361, 117724, 82973, 99750, 101682, 103031, 33091, 9554, 48706, 1811, 115023, 8239, 235, 101538, 34547, 29172, 115889, 57668, 106310, 19000, 36117, 251, 33035, 16325, 31634, 118551, 106142, 21688, 250, 34208, 22649, 105487, 113415, 106483, 50021, 113797, 30358, 104314, 24273, 9554, 37689, 91774, 9174, 109311, 23897, 117986, 34208, 5486, 105469, 19361, 113312, 105085, 48634, 103786, 117681, 107875, 17792, 64209, 9554, 102158, 104873, 116748, 50338, 121103, 105000, 66201, 57668, 97655, 101399, 15120, 25129, 89151, 106340, 9554, 102993, 120824, 30046, 116748, 28190, 104314, 115070, 106499, 108327, 108928, 11239, 250, 9174, 112599, 3922, 109311, 111671, 110310, 50338, 121103, 1811, 118835, 118835, 1811, 128009, 128006, 882, 128007, 271, 62543, 48044, 89186, 9554, 113511, 73981, 106808, 198, 103864, 38574, 105532, 108826, 65571, 26592, 50928, 103721, 120, 115344, 11571, 128009, 128006, 78191, 128007, 271, 31374, 248, 17792, 107226, 105714, 106691, 37026, 103106, 34171, 23039, 101399, 9554, 104241, 104295, 38574, 111923, 101171, 104, 107611, 104091, 121769, 107328, 9554, 9080, 45829, 121591, 16555, 94, 40474, 72843, 101682, 102701, 25580, 9080, 45829, 105067, 38574, 39177, 101426, 1811, 53901, 112798, 124162, 106258, 113871, 101067, 102836, 26955, 246, 119147, 35304, 109365, 105156, 9554, 114079, 104611, 3922, 111764, 124931, 54322, 3922, 104157, 43568, 23187, 32938, 104295, 102769, 110039, 75146, 17920, 236, 20834, 103106, 1811, 106763, 57668, 41642, 114876, 104123, 72368, 104314, 107611, 30926, 29172, 103181, 120544, 127762, 104660, 127035, 76537, 70203, 86436, 112798, 103864, 102159, 124162, 39177, 3922, 31374, 248, 17792, 104241, 88367, 81258, 101835, 20834, 102321, 1811, 31374, 248, 17792, 109163, 20379, 33035, 104198, 19653, 55030, 47095, 21043, 104150, 68171, 37026, 103106, 9554, 123120, 104122, 101544, 23538, 9554, 125113, 108630, 21043, 106812, 102993, 107381, 1811, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "你現在是一位經驗豐富的塔羅牌師，擁有數十年的解牌經驗和深厚的直覺洞察力。請仔細查看附加的CSV檔案，其中包含100個不同的牌組（Spread Card Composition）。 你的任務是：\n",
            "1. 仔細閱讀每個牌組中塔羅牌的名稱，以及欄位 Spread Card Composition其正位和逆位的含義。\n",
            "2. 針對\"Overall Love Spread Meaning\"欄位，運用你豐富的塔羅經驗和洞察力，重新進行深入解讀。這個解讀應該：\n",
            "   - 超越原文的表面含義，挖掘更深層的洞見\n",
            "   - 融入你作為資深塔羅牌師的獨特見解\n",
            "   - 加入原文未提及但相關的解說，豐富整體解讀\n",
            "   - 考慮牌陣中各張牌之間的相互關係和能量流動\n",
            "   - 提供更具體、實用的建議給尋求指引的人\n",
            "3. 從Combination Number 3530025開始，依次解讀至3530050，共25組牌陣。\n",
            "4. 每組解讀應包含：\n",
            "   - 牌陣編號(註明)\n",
            "   - 牌組組成(註明)\n",
            "   - 你的詳細解讀（包含實用的建議與解讀共約200字,共分2段解讀與建議）\n",
            "5. 在解讀時，請考慮以下幾點：\n",
            "   - 牌陣中各張牌的位置和相互關係\n",
            "   - 正逆位的影響\n",
            "   - 實用的建議(與牌面相近要有意思區隔的建議)或行動指南\n",
            "6.文字呈現範例如下,請依照此格式輸出  \n",
            "牌陣編號: 3530001\n",
            "牌組組成: 權杖八 (正位)、寶劍八 (逆位)、寶劍王后 (正位)\n",
            "詳細解讀：\n",
            "權杖八的正位顯示你目前的感情狀態充滿了快速且正面的變化，這與寶劍八逆位的釋放束縛相呼應，暗示著你們正在或即將從一段困境中解脫。這兩張牌共同表示你們的關係將由束縛中解放，進而進入更自由、開明的階段。寶劍王后的理智與洞察力將在這過程中起到關鍵作用，使你們能在情感上更加成熟和理性。\n",
            "建議與指引：\n",
            "建議你們利用這段契機重新審視彼此的需求與目標，並以開放的態度迎接新的開始。實際上，權杖八暗示旅行或變化，或許也可以考慮來一段假期，讓雙方有更多時間感受彼此的存在。寶劍王后提醒你們在溝通中要保持冷靜和理智，並互相尊重對方的意見。\n",
            "請以溫和、富有洞察力且鼓舞人心的語調進行解讀，就像你正在為一位真實的來訪者進行面對面的塔羅占卜。\n",
            "現在，請開始你的解讀。謝謝。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "写一个有效的比较语句\n",
            "這段關係是否值得繼續？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "愚人是一張代表自發性行為的牌，一段跳脫某種狀態的日子，或盡情享受眼前日子的一段時光。好冒險，有夢想，不拘泥于傳統的觀念，自由奔放，居無定所，一切從基礎出發。當你周遭的人都對某事提防戒慎，你卻打算去冒這個險時，愚人牌可能就會出現。愚人暗示通往成功之路是經由自發的行動，而長期的計劃則是將來的事。<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 31374, 248, 17792, 107226, 105714, 106691, 37026, 103106, 34171, 23039, 101399, 9554, 104241, 104295, 38574, 111923, 101171, 104, 107611, 104091, 121769, 107328, 9554, 9080, 45829, 121591, 16555, 94, 40474, 72843, 101682, 102701, 25580, 9080, 45829, 105067, 38574, 39177, 101426, 1811, 53901, 112798, 124162, 106258, 113871, 101067, 102836, 26955, 246, 119147, 35304, 109365, 105156, 9554, 114079, 104611, 3922, 111764, 124931, 54322, 3922, 104157, 43568, 23187, 32938, 104295, 102769, 110039, 75146, 17920, 236, 20834, 103106, 1811, 106763, 57668, 41642, 114876, 104123, 72368, 104314, 107611, 30926, 29172, 103181, 120544, 127762, 104660, 127035, 76537, 70203, 86436, 112798, 103864, 102159, 124162, 39177, 3922, 31374, 248, 17792, 104241, 88367, 81258, 101835, 20834, 102321, 1811, 31374, 248, 17792, 109163, 20379, 33035, 104198, 19653, 55030, 47095, 21043, 104150, 68171, 37026, 103106, 9554, 123120, 104122, 101544, 23538, 9554, 125113, 108630, 21043, 106812, 102993, 107381, 1811, 128009]\n",
            "labels:\n",
            "愚人是一張代表自發性行為的牌，一段跳脫某種狀態的日子，或盡情享受眼前日子的一段時光。好冒險，有夢想，不拘泥于傳統的觀念，自由奔放，居無定所，一切從基礎出發。當你周遭的人都對某事提防戒慎，你卻打算去冒這個險時，愚人牌可能就會出現。愚人暗示通往成功之路是經由自發的行動，而長期的計劃則是將來的事。<|eot_id|>\n",
            "config.json: 100% 649/649 [00:00<00:00, 3.46MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-08-21 03:12:44,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-21 03:12:44,182 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"shenzhi-wang/Llama3-8B-Chinese-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "08/21/2024 03:12:44 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 78.6MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-08-21 03:12:45,340 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/4.98G [00:00<03:27, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 21.0M/4.98G [00:00<03:25, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 31.5M/4.98G [00:01<03:22, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 41.9M/4.98G [00:01<03:22, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 52.4M/4.98G [00:02<03:22, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 62.9M/4.98G [00:02<03:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 73.4M/4.98G [00:03<03:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 83.9M/4.98G [00:03<03:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 94.4M/4.98G [00:03<03:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 105M/4.98G [00:04<03:20, 24.3MB/s] \u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 115M/4.98G [00:04<03:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 126M/4.98G [00:05<03:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 136M/4.98G [00:05<03:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 147M/4.98G [00:06<03:18, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 157M/4.98G [00:06<03:18, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 168M/4.98G [00:06<03:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 178M/4.98G [00:07<03:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 189M/4.98G [00:07<03:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 199M/4.98G [00:08<03:16, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 210M/4.98G [00:08<03:16, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 220M/4.98G [00:09<03:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 231M/4.98G [00:09<03:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 241M/4.98G [00:09<03:16, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 252M/4.98G [00:10<03:18, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 262M/4.98G [00:10<03:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 273M/4.98G [00:11<03:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 283M/4.98G [00:11<03:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 294M/4.98G [00:12<03:12, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 304M/4.98G [00:12<03:12, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 315M/4.98G [00:12<03:11, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 325M/4.98G [00:13<03:11, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 336M/4.98G [00:13<03:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 346M/4.98G [00:14<03:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 357M/4.98G [00:14<03:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 367M/4.98G [00:15<03:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 377M/4.98G [00:15<03:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 388M/4.98G [00:15<03:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 398M/4.98G [00:16<03:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 409M/4.98G [00:16<03:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 419M/4.98G [00:17<03:07, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 430M/4.98G [00:17<03:07, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 440M/4.98G [00:18<03:06, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 451M/4.98G [00:18<03:06, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 461M/4.98G [00:19<03:05, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 472M/4.98G [00:19<03:06, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 482M/4.98G [00:19<03:06, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 493M/4.98G [00:20<03:05, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 503M/4.98G [00:20<03:08, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 514M/4.98G [00:21<03:18, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 524M/4.98G [00:21<03:13, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 535M/4.98G [00:22<03:10, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 545M/4.98G [00:22<03:08, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 556M/4.98G [00:23<03:08, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 566M/4.98G [00:23<03:06, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 577M/4.98G [00:23<03:05, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 587M/4.98G [00:24<03:03, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 598M/4.98G [00:24<03:02, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 608M/4.98G [00:25<03:01, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 619M/4.98G [00:25<03:00, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 629M/4.98G [00:26<02:59, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 640M/4.98G [00:26<02:58, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 650M/4.98G [00:26<02:58, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 661M/4.98G [00:27<02:58, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 671M/4.98G [00:27<02:58, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 682M/4.98G [00:28<02:57, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 692M/4.98G [00:28<02:56, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 703M/4.98G [00:29<02:56, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 713M/4.98G [00:29<02:55, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 724M/4.98G [00:29<02:55, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 734M/4.98G [00:30<02:55, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 744M/4.98G [00:30<02:54, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 755M/4.98G [00:31<02:54, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 765M/4.98G [00:31<02:55, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 776M/4.98G [00:32<02:54, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 786M/4.98G [00:32<02:53, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 797M/4.98G [00:33<02:52, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 807M/4.98G [00:33<02:52, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 818M/4.98G [00:33<02:51, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 828M/4.98G [00:34<02:51, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 839M/4.98G [00:34<02:50, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 849M/4.98G [00:35<02:50, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 860M/4.98G [00:35<02:49, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 870M/4.98G [00:36<02:49, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 881M/4.98G [00:36<02:48, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 891M/4.98G [00:36<02:48, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 902M/4.98G [00:37<02:58, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 912M/4.98G [00:37<02:54, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 923M/4.98G [00:38<02:52, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 933M/4.98G [00:38<02:50, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 944M/4.98G [00:39<02:50, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 954M/4.98G [00:39<02:48, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 965M/4.98G [00:40<02:47, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 975M/4.98G [00:40<02:46, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 986M/4.98G [00:40<02:45, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 996M/4.98G [00:41<02:45, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [00:41<02:44, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.02G/4.98G [00:42<02:44, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.03G/4.98G [00:42<02:44, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.04G/4.98G [00:43<02:43, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.05G/4.98G [00:43<02:43, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.06G/4.98G [00:43<02:42, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.07G/4.98G [00:44<02:42, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.08G/4.98G [00:44<02:42, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.09G/4.98G [00:45<02:41, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.10G/4.98G [00:45<02:40, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.11G/4.98G [00:46<02:40, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.12G/4.98G [00:46<02:39, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.13G/4.98G [00:46<02:38, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:47<02:38, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.15G/4.98G [00:47<02:37, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.16G/4.98G [00:48<02:37, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.17G/4.98G [00:48<02:37, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.18G/4.98G [00:49<02:36, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.20G/4.98G [00:49<02:36, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.21G/4.98G [00:50<02:35, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.22G/4.98G [00:50<02:35, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.23G/4.98G [00:50<02:34, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:51<02:34, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.25G/4.98G [00:51<02:34, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.26G/4.98G [00:52<02:33, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.27G/4.98G [00:52<02:32, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [00:53<02:32, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.29G/4.98G [00:53<02:35, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.30G/4.98G [00:53<02:39, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.31G/4.98G [00:54<02:36, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.32G/4.98G [00:54<02:34, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.33G/4.98G [00:55<02:33, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.34G/4.98G [00:55<02:31, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.35G/4.98G [00:56<02:31, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.36G/4.98G [00:56<02:30, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.37G/4.98G [00:57<02:31, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.38G/4.98G [00:57<02:30, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.39G/4.98G [00:57<02:29, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.41G/4.98G [00:58<02:28, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.42G/4.98G [00:58<02:28, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.43G/4.98G [00:59<02:27, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.44G/4.98G [00:59<02:26, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.45G/4.98G [01:00<02:25, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.46G/4.98G [01:00<02:25, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.47G/4.98G [01:00<02:25, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [01:01<02:25, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.49G/4.98G [01:01<02:25, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.50G/4.98G [01:02<02:50, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.51G/4.98G [01:02<02:41, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.52G/4.98G [01:03<02:35, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.53G/4.98G [01:03<02:30, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.54G/4.98G [01:04<02:27, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.55G/4.98G [01:04<02:25, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.56G/4.98G [01:05<02:23, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.57G/4.98G [01:05<02:22, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.58G/4.98G [01:05<02:21, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.59G/4.98G [01:06<02:20, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.60G/4.98G [01:06<02:19, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.61G/4.98G [01:07<02:18, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.63G/4.98G [01:07<02:18, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.64G/4.98G [01:08<02:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.65G/4.98G [01:08<02:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.66G/4.98G [01:08<02:16, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.67G/4.98G [01:09<02:16, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [01:09<02:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.69G/4.98G [01:10<02:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.70G/4.98G [01:10<02:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.71G/4.98G [01:11<02:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.72G/4.98G [01:11<02:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.73G/4.98G [01:12<02:14, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.74G/4.98G [01:12<02:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [01:12<02:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.76G/4.98G [01:13<02:11, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.77G/4.98G [01:13<02:11, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.78G/4.98G [01:14<02:11, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.79G/4.98G [01:14<02:10, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.80G/4.98G [01:15<02:10, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.81G/4.98G [01:15<02:09, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.82G/4.98G [01:15<02:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.84G/4.98G [01:16<02:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.85G/4.98G [01:16<02:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.86G/4.98G [01:17<02:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.87G/4.98G [01:17<02:07, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.88G/4.98G [01:18<02:07, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.89G/4.98G [01:18<02:06, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.90G/4.98G [01:18<02:06, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.91G/4.98G [01:19<02:06, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.92G/4.98G [01:19<02:05, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.93G/4.98G [01:20<02:05, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.94G/4.98G [01:20<02:05, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [01:21<02:04, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [01:21<02:03, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.97G/4.98G [01:21<02:03, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.98G/4.98G [01:22<02:03, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.99G/4.98G [01:22<02:02, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.00G/4.98G [01:23<02:06, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.01G/4.98G [01:23<02:00, 24.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.02G/4.98G [01:24<02:00, 24.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.03G/4.98G [01:24<02:00, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.04G/4.98G [01:24<02:00, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.06G/4.98G [01:25<01:59, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.07G/4.98G [01:25<01:59, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.08G/4.98G [01:26<01:59, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.09G/4.98G [01:26<01:58, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.10G/4.98G [01:27<01:58, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.11G/4.98G [01:27<01:57, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.12G/4.98G [01:28<02:09, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.13G/4.98G [01:28<02:05, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.14G/4.98G [01:28<02:03, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [01:29<02:01, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.16G/4.98G [01:29<01:59, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.17G/4.98G [01:30<01:58, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.18G/4.98G [01:30<01:57, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.19G/4.98G [01:31<01:55, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.20G/4.98G [01:31<01:55, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.21G/4.98G [01:32<01:54, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.22G/4.98G [01:32<01:54, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.23G/4.98G [01:32<01:53, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.24G/4.98G [01:33<01:52, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.25G/4.98G [01:33<01:52, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.26G/4.98G [01:34<01:56, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.28G/4.98G [01:34<02:10, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.29G/4.98G [01:35<02:04, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.30G/4.98G [01:35<01:59, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.31G/4.98G [01:36<01:56, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.32G/4.98G [01:36<01:53, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.33G/4.98G [01:37<01:52, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.34G/4.98G [01:37<01:50, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [01:37<01:49, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.36G/4.98G [01:38<01:48, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.37G/4.98G [01:38<01:47, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.38G/4.98G [01:39<01:47, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.39G/4.98G [01:39<01:46, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.40G/4.98G [01:40<01:46, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.41G/4.98G [01:40<01:45, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.42G/4.98G [01:40<01:45, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.43G/4.98G [01:41<01:44, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.44G/4.98G [01:41<01:44, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.45G/4.98G [01:42<01:43, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.46G/4.98G [01:42<01:43, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.47G/4.98G [01:43<01:42, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.49G/4.98G [01:43<01:42, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.50G/4.98G [01:43<01:43, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.51G/4.98G [01:44<01:40, 24.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.52G/4.98G [01:44<01:40, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.53G/4.98G [01:45<01:40, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.54G/4.98G [01:45<01:40, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.55G/4.98G [01:46<01:39, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.56G/4.98G [01:46<01:39, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.57G/4.98G [01:47<01:43, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.58G/4.98G [01:47<01:37, 24.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.59G/4.98G [01:47<01:37, 24.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.60G/4.98G [01:48<01:36, 24.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.61G/4.98G [01:48<01:36, 24.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [01:49<01:36, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.63G/4.98G [01:49<01:40, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.64G/4.98G [01:50<01:38, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.65G/4.98G [01:50<01:37, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.66G/4.98G [01:50<01:36, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.67G/4.98G [01:51<01:35, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.68G/4.98G [01:51<01:44, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.69G/4.98G [01:52<01:40, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.71G/4.98G [01:52<01:38, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.72G/4.98G [01:53<01:36, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.73G/4.98G [01:53<01:34, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.74G/4.98G [01:54<01:33, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.75G/4.98G [01:54<01:32, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.76G/4.98G [01:54<01:31, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.77G/4.98G [01:55<01:31, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.78G/4.98G [01:55<01:31, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.79G/4.98G [01:56<01:30, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.80G/4.98G [01:56<01:29, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.81G/4.98G [01:57<01:29, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.82G/4.98G [01:57<01:28, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.83G/4.98G [01:57<01:28, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [01:58<01:27, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.85G/4.98G [01:58<01:27, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.86G/4.98G [01:59<01:26, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.87G/4.98G [01:59<01:26, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.88G/4.98G [02:00<01:26, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [02:00<01:25, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.90G/4.98G [02:00<01:25, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.92G/4.98G [02:01<01:24, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.93G/4.98G [02:01<01:24, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.94G/4.98G [02:02<01:24, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.95G/4.98G [02:02<01:23, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.96G/4.98G [02:03<01:23, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.97G/4.98G [02:03<01:22, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.98G/4.98G [02:03<01:22, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.99G/4.98G [02:04<01:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.00G/4.98G [02:04<01:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.01G/4.98G [02:05<01:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.02G/4.98G [02:05<01:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.03G/4.98G [02:06<01:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.04G/4.98G [02:06<01:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.05G/4.98G [02:07<01:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.06G/4.98G [02:07<01:18, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.07G/4.98G [02:07<01:18, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.08G/4.98G [02:08<01:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [02:08<01:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.10G/4.98G [02:09<01:17, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.11G/4.98G [02:09<01:16, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.12G/4.98G [02:10<01:16, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.14G/4.98G [02:10<01:16, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.15G/4.98G [02:10<01:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.16G/4.98G [02:11<01:15, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.17G/4.98G [02:11<01:24, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.18G/4.98G [02:12<01:23, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.19G/4.98G [02:13<01:30, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.20G/4.98G [02:13<01:34, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.21G/4.98G [02:14<01:29, 19.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [02:14<01:33, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.23G/4.98G [02:15<01:27, 19.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.24G/4.98G [02:15<01:32, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.25G/4.98G [02:16<01:26, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.26G/4.98G [02:16<01:30, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.27G/4.98G [02:17<01:24, 20.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.28G/4.98G [02:17<01:28, 19.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.29G/4.98G [02:18<01:23, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.30G/4.98G [02:18<01:19, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.31G/4.98G [02:19<01:24, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.32G/4.98G [02:19<01:19, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.33G/4.98G [02:20<01:16, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.34G/4.98G [02:20<01:21, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.36G/4.98G [02:21<01:17, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.37G/4.98G [02:21<01:14, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.38G/4.98G [02:22<01:19, 20.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.39G/4.98G [02:22<01:16, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.40G/4.98G [02:23<01:13, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.41G/4.98G [02:24<01:17, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.42G/4.98G [02:24<01:14, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.43G/4.98G [02:24<01:12, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [02:25<01:15, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.45G/4.98G [02:25<01:12, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.46G/4.98G [02:26<01:10, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.47G/4.98G [02:27<01:14, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.48G/4.98G [02:27<01:11, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.49G/4.98G [02:27<01:09, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.50G/4.98G [02:28<01:07, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.51G/4.98G [02:28<01:11, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.52G/4.98G [02:29<01:07, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.53G/4.98G [02:29<01:06, 21.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.54G/4.98G [02:30<01:04, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.55G/4.98G [02:30<01:08, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.57G/4.98G [02:31<01:05, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.58G/4.98G [02:31<01:03, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.59G/4.98G [02:32<01:01, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.60G/4.98G [02:32<01:00, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.61G/4.98G [02:33<00:59, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.62G/4.98G [02:33<00:57, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.63G/4.98G [02:34<01:00, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.64G/4.98G [02:34<00:59, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.65G/4.98G [02:34<00:57, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.66G/4.98G [02:35<00:56, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.67G/4.98G [02:35<00:55, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.68G/4.98G [02:36<00:54, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.69G/4.98G [02:36<01:02, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.70G/4.98G [02:37<00:59, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.71G/4.98G [02:38<01:04, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.72G/4.98G [02:38<01:07, 18.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.73G/4.98G [02:39<01:02, 19.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.74G/4.98G [02:39<00:58, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.75G/4.98G [02:40<01:02, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.76G/4.98G [02:40<00:58, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.77G/4.98G [02:41<00:55, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.79G/4.98G [02:41<00:53, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.80G/4.98G [02:42<01:06, 17.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.81G/4.98G [02:42<01:07, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.82G/4.98G [02:43<01:07, 17.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.83G/4.98G [02:44<01:09, 16.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.84G/4.98G [02:44<01:09, 16.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.85G/4.98G [02:45<01:15, 14.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.86G/4.98G [02:46<01:19, 14.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.87G/4.98G [02:47<01:22, 13.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.88G/4.98G [02:48<01:25, 12.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.89G/4.98G [02:49<01:32, 11.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.90G/4.98G [02:50<01:30, 11.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.91G/4.98G [02:51<01:34, 11.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.92G/4.98G [02:52<01:32, 11.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.93G/4.98G [02:53<01:29, 11.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.94G/4.98G [02:53<01:27, 11.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.95G/4.98G [02:54<01:26, 11.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.96G/4.98G [02:55<01:24, 12.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.97G/4.98G [02:56<01:28, 11.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.98G/4.98G [02:57<01:25, 11.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 4.00G/4.98G [02:58<01:24, 11.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 4.01G/4.98G [02:59<01:22, 11.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.02G/4.98G [03:00<01:20, 11.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.03G/4.98G [03:01<01:19, 12.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.04G/4.98G [03:01<01:18, 12.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.05G/4.98G [03:02<01:16, 12.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.06G/4.98G [03:03<01:15, 12.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.07G/4.98G [03:04<01:14, 12.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.08G/4.98G [03:05<01:08, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.09G/4.98G [03:06<01:09, 12.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.10G/4.98G [03:06<01:04, 13.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.11G/4.98G [03:07<01:04, 13.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.12G/4.98G [03:08<01:00, 14.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.13G/4.98G [03:08<00:57, 14.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.14G/4.98G [03:09<00:50, 16.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.15G/4.98G [03:09<00:50, 16.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.16G/4.98G [03:10<00:49, 16.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.17G/4.98G [03:10<00:44, 18.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.18G/4.98G [03:11<00:44, 17.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.19G/4.98G [03:12<00:40, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.20G/4.98G [03:12<00:37, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.22G/4.98G [03:12<00:35, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.23G/4.98G [03:13<00:33, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.24G/4.98G [03:13<00:32, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.25G/4.98G [03:14<00:31, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.26G/4.98G [03:14<00:30, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.27G/4.98G [03:15<00:29, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.28G/4.98G [03:15<00:29, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.29G/4.98G [03:15<00:28, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.30G/4.98G [03:16<00:28, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.31G/4.98G [03:16<00:27, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.32G/4.98G [03:17<00:27, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.33G/4.98G [03:17<00:26, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.34G/4.98G [03:18<00:26, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.35G/4.98G [03:18<00:25, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.36G/4.98G [03:18<00:25, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.37G/4.98G [03:19<00:24, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.38G/4.98G [03:19<00:25, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.39G/4.98G [03:20<00:23, 24.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.40G/4.98G [03:20<00:23, 24.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.41G/4.98G [03:21<00:23, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.42G/4.98G [03:21<00:22, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.44G/4.98G [03:21<00:22, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.45G/4.98G [03:22<00:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.46G/4.98G [03:22<00:21, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.47G/4.98G [03:23<00:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.48G/4.98G [03:23<00:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.49G/4.98G [03:24<00:20, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.50G/4.98G [03:24<00:19, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [03:25<00:19, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.52G/4.98G [03:25<00:19, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.53G/4.98G [03:25<00:18, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.54G/4.98G [03:26<00:18, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.55G/4.98G [03:26<00:17, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.56G/4.98G [03:27<00:17, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.57G/4.98G [03:27<00:17, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.58G/4.98G [03:28<00:16, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.59G/4.98G [03:28<00:16, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.60G/4.98G [03:29<00:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.61G/4.98G [03:29<00:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.62G/4.98G [03:29<00:14, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.63G/4.98G [03:30<00:14, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.65G/4.98G [03:30<00:13, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.66G/4.98G [03:31<00:13, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.67G/4.98G [03:31<00:12, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.68G/4.98G [03:32<00:12, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.69G/4.98G [03:32<00:11, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.70G/4.98G [03:32<00:11, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.71G/4.98G [03:33<00:11, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.72G/4.98G [03:33<00:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.73G/4.98G [03:34<00:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.74G/4.98G [03:34<00:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.75G/4.98G [03:35<00:09, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.76G/4.98G [03:35<00:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.77G/4.98G [03:35<00:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.78G/4.98G [03:36<00:08, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.79G/4.98G [03:36<00:07, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.80G/4.98G [03:37<00:07, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.81G/4.98G [03:37<00:07, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.82G/4.98G [03:38<00:07, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.83G/4.98G [03:38<00:06, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.84G/4.98G [03:39<00:06, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.85G/4.98G [03:40<00:07, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.87G/4.98G [03:40<00:06, 17.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.88G/4.98G [03:42<00:07, 12.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.89G/4.98G [03:43<00:08, 10.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.90G/4.98G [03:45<00:08, 9.43MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.91G/4.98G [03:46<00:07, 8.99MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.92G/4.98G [03:47<00:06, 8.73MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.93G/4.98G [03:48<00:05, 8.56MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.94G/4.98G [03:50<00:04, 8.46MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.95G/4.98G [03:51<00:03, 8.35MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.96G/4.98G [03:53<00:02, 7.71MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.97G/4.98G [03:54<00:00, 7.60MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [03:55<00:00, 21.1MB/s]\n",
            "Downloading shards:  25% 1/4 [03:55<11:47, 235.88s/it]\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   0% 10.5M/5.00G [00:00<03:32, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   0% 21.0M/5.00G [00:00<03:31, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 31.5M/5.00G [00:01<03:31, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 41.9M/5.00G [00:01<03:30, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 52.4M/5.00G [00:02<03:29, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 62.9M/5.00G [00:02<03:29, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 73.4M/5.00G [00:03<03:28, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 83.9M/5.00G [00:03<03:28, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 94.4M/5.00G [00:04<03:28, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 105M/5.00G [00:04<03:27, 23.6MB/s] \u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 115M/5.00G [00:04<03:27, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 126M/5.00G [00:05<03:26, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 136M/5.00G [00:05<03:26, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 147M/5.00G [00:06<03:25, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 157M/5.00G [00:06<03:25, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 168M/5.00G [00:07<03:25, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 178M/5.00G [00:07<03:24, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 189M/5.00G [00:08<03:24, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 199M/5.00G [00:08<03:23, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 210M/5.00G [00:08<03:23, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 220M/5.00G [00:09<03:22, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 231M/5.00G [00:09<03:22, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 241M/5.00G [00:10<03:25, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 252M/5.00G [00:10<03:21, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 262M/5.00G [00:11<03:20, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 273M/5.00G [00:11<03:20, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 283M/5.00G [00:12<03:20, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 294M/5.00G [00:12<03:19, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 304M/5.00G [00:12<03:19, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 315M/5.00G [00:13<03:18, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 325M/5.00G [00:13<03:18, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 336M/5.00G [00:14<03:17, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 346M/5.00G [00:14<03:17, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 357M/5.00G [00:15<03:17, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 367M/5.00G [00:15<03:16, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 377M/5.00G [00:16<03:16, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 388M/5.00G [00:16<03:16, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 398M/5.00G [00:17<03:45, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 409M/5.00G [00:17<03:35, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 419M/5.00G [00:18<03:29, 21.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 430M/5.00G [00:18<03:24, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 440M/5.00G [00:18<03:20, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 451M/5.00G [00:19<03:18, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 461M/5.00G [00:19<03:15, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 472M/5.00G [00:20<03:14, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 482M/5.00G [00:20<03:14, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 493M/5.00G [00:21<03:11, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 503M/5.00G [00:21<03:11, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 514M/5.00G [00:22<03:10, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 524M/5.00G [00:22<03:10, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 535M/5.00G [00:22<03:09, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 545M/5.00G [00:23<03:09, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 556M/5.00G [00:23<03:08, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 566M/5.00G [00:24<03:08, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 577M/5.00G [00:24<03:07, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 587M/5.00G [00:25<03:07, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 598M/5.00G [00:25<03:06, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 608M/5.00G [00:26<03:06, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 619M/5.00G [00:26<03:06, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 629M/5.00G [00:26<03:05, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 640M/5.00G [00:27<03:06, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 650M/5.00G [00:27<03:05, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 661M/5.00G [00:28<03:04, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 671M/5.00G [00:28<03:04, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 682M/5.00G [00:29<03:03, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 692M/5.00G [00:29<03:03, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 703M/5.00G [00:30<03:02, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 713M/5.00G [00:30<03:02, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 724M/5.00G [00:30<03:02, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 734M/5.00G [00:31<03:00, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 744M/5.00G [00:31<03:00, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 755M/5.00G [00:32<02:59, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 765M/5.00G [00:32<02:59, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 776M/5.00G [00:33<02:59, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 786M/5.00G [00:33<02:58, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 797M/5.00G [00:34<02:58, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 807M/5.00G [00:34<02:58, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 818M/5.00G [00:34<02:57, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 828M/5.00G [00:35<02:57, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 839M/5.00G [00:36<03:15, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 849M/5.00G [00:36<03:09, 21.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 860M/5.00G [00:36<03:04, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 870M/5.00G [00:37<03:01, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 881M/5.00G [00:37<02:59, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 891M/5.00G [00:38<02:57, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 902M/5.00G [00:38<02:58, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 912M/5.00G [00:39<03:27, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 923M/5.00G [00:39<02:43, 24.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 933M/5.00G [00:40<02:46, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 944M/5.00G [00:40<02:47, 24.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 954M/5.00G [00:40<02:48, 24.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 965M/5.00G [00:41<02:51, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 975M/5.00G [00:41<02:48, 23.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 986M/5.00G [00:42<02:52, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 996M/5.00G [00:42<03:15, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 1.01G/5.00G [00:43<03:07, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 1.02G/5.00G [00:43<03:01, 21.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.03G/5.00G [00:44<02:57, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.04G/5.00G [00:44<02:54, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.05G/5.00G [00:45<02:51, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.06G/5.00G [00:45<02:50, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.07G/5.00G [00:46<02:48, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.08G/5.00G [00:46<02:47, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.09G/5.00G [00:46<02:46, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.10G/5.00G [00:47<02:46, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.11G/5.00G [00:47<02:45, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.12G/5.00G [00:48<02:44, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.13G/5.00G [00:48<02:44, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:49<02:43, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.15G/5.00G [00:49<02:43, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.16G/5.00G [00:50<02:42, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.17G/5.00G [00:50<02:42, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.18G/5.00G [00:50<02:41, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.20G/5.00G [00:51<02:41, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.21G/5.00G [00:51<02:44, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.22G/5.00G [00:52<02:39, 23.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.23G/5.00G [00:52<02:39, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.24G/5.00G [00:53<02:39, 23.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.25G/5.00G [00:53<02:38, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.26G/5.00G [00:54<02:38, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.27G/5.00G [00:54<02:38, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.28G/5.00G [00:55<03:01, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.29G/5.00G [00:55<03:16, 18.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.30G/5.00G [00:56<03:05, 19.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.31G/5.00G [00:56<03:19, 18.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.32G/5.00G [00:57<03:07, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.33G/5.00G [00:58<03:19, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.34G/5.00G [00:58<03:06, 19.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.35G/5.00G [00:59<03:18, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.36G/5.00G [00:59<03:06, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.37G/5.00G [01:00<03:18, 18.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.38G/5.00G [01:00<03:05, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.39G/5.00G [01:01<02:56, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [01:01<03:10, 18.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.42G/5.00G [01:02<02:58, 20.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.43G/5.00G [01:02<02:51, 20.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.44G/5.00G [01:03<03:05, 19.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.45G/5.00G [01:03<02:55, 20.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.46G/5.00G [01:04<03:08, 18.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.47G/5.00G [01:04<02:57, 19.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.48G/5.00G [01:05<02:49, 20.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.49G/5.00G [01:05<02:43, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.50G/5.00G [01:06<02:59, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.51G/5.00G [01:06<02:50, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.52G/5.00G [01:07<02:43, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.53G/5.00G [01:08<02:58, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.54G/5.00G [01:08<02:49, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.55G/5.00G [01:08<02:42, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.56G/5.00G [01:09<02:57, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.57G/5.00G [01:10<02:48, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.58G/5.00G [01:10<02:41, 21.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.59G/5.00G [01:11<02:55, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.60G/5.00G [01:11<02:47, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.61G/5.00G [01:12<02:41, 20.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.63G/5.00G [01:12<02:35, 21.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.64G/5.00G [01:13<02:49, 19.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.65G/5.00G [01:13<02:42, 20.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.66G/5.00G [01:14<02:36, 21.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.67G/5.00G [01:14<02:32, 21.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [01:14<02:28, 22.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.69G/5.00G [01:15<02:35, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.70G/5.00G [01:16<02:38, 20.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.71G/5.00G [01:16<02:33, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.72G/5.00G [01:16<02:30, 21.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.73G/5.00G [01:17<02:25, 22.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.74G/5.00G [01:17<02:22, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.75G/5.00G [01:18<02:20, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.76G/5.00G [01:18<02:19, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.77G/5.00G [01:19<02:18, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.78G/5.00G [01:19<02:17, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.79G/5.00G [01:20<02:17, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.80G/5.00G [01:20<02:16, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [01:20<02:15, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.82G/5.00G [01:21<02:15, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.84G/5.00G [01:21<02:14, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.85G/5.00G [01:22<02:14, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.86G/5.00G [01:22<02:16, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.87G/5.00G [01:23<02:34, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.88G/5.00G [01:24<02:46, 18.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.89G/5.00G [01:24<02:37, 19.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.90G/5.00G [01:25<02:47, 18.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.91G/5.00G [01:25<02:37, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.92G/5.00G [01:26<02:29, 20.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.93G/5.00G [01:26<02:41, 19.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.94G/5.00G [01:27<02:31, 20.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [01:27<02:25, 21.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.96G/5.00G [01:28<02:20, 21.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.97G/5.00G [01:28<02:16, 22.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 1.98G/5.00G [01:29<02:14, 22.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 1.99G/5.00G [01:29<02:12, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.00G/5.00G [01:29<02:13, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.01G/5.00G [01:30<02:24, 20.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.02G/5.00G [01:30<02:18, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.03G/5.00G [01:31<02:14, 22.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.04G/5.00G [01:31<02:11, 22.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.06G/5.00G [01:32<02:09, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.07G/5.00G [01:32<02:07, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.08G/5.00G [01:33<02:06, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.09G/5.00G [01:33<02:04, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.10G/5.00G [01:34<02:22, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.11G/5.00G [01:34<02:34, 18.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.12G/5.00G [01:35<02:26, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.13G/5.00G [01:36<02:35, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.14G/5.00G [01:36<02:42, 17.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.15G/5.00G [01:37<02:30, 18.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.16G/5.00G [01:37<02:38, 18.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.17G/5.00G [01:38<02:27, 19.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.18G/5.00G [01:38<02:35, 18.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.19G/5.00G [01:39<02:25, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.20G/5.00G [01:39<02:17, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.21G/5.00G [01:40<02:30, 18.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.22G/5.00G [01:41<02:20, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.23G/5.00G [01:41<02:31, 18.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.24G/5.00G [01:42<02:38, 17.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.25G/5.00G [01:43<02:43, 16.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.26G/5.00G [01:43<02:46, 16.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.28G/5.00G [01:44<02:48, 16.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.29G/5.00G [01:45<02:49, 16.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.30G/5.00G [01:45<02:49, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.31G/5.00G [01:46<02:49, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.32G/5.00G [01:47<02:48, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.33G/5.00G [01:47<02:48, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.34G/5.00G [01:48<02:47, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.35G/5.00G [01:49<02:46, 15.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.36G/5.00G [01:49<02:32, 17.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.37G/5.00G [01:50<02:35, 16.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.38G/5.00G [01:50<02:37, 16.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.39G/5.00G [01:51<02:39, 16.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.40G/5.00G [01:52<02:40, 16.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.41G/5.00G [01:52<02:26, 17.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [01:53<02:30, 17.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.43G/5.00G [01:53<02:33, 16.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.44G/5.00G [01:54<02:35, 16.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.45G/5.00G [01:55<02:25, 17.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.46G/5.00G [01:55<02:26, 17.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.47G/5.00G [01:56<02:29, 16.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.49G/5.00G [01:57<02:32, 16.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.50G/5.00G [01:57<02:49, 14.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.51G/5.00G [01:58<02:46, 15.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.52G/5.00G [01:59<02:58, 13.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.53G/5.00G [02:00<02:52, 14.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.54G/5.00G [02:00<02:47, 14.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [02:01<02:43, 15.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.56G/5.00G [02:02<02:40, 15.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.57G/5.00G [02:02<02:37, 15.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.58G/5.00G [02:03<02:36, 15.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.59G/5.00G [02:04<02:34, 15.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.60G/5.00G [02:04<02:19, 17.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.61G/5.00G [02:05<02:22, 16.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.62G/5.00G [02:05<02:24, 16.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.63G/5.00G [02:06<02:25, 16.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.64G/5.00G [02:07<02:25, 16.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.65G/5.00G [02:07<02:12, 17.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.66G/5.00G [02:08<02:16, 17.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.67G/5.00G [02:09<02:18, 16.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.68G/5.00G [02:09<02:20, 16.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.69G/5.00G [02:10<02:08, 17.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.71G/5.00G [02:10<02:13, 17.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.72G/5.00G [02:11<02:15, 16.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.73G/5.00G [02:12<02:17, 16.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.74G/5.00G [02:12<02:06, 17.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.75G/5.00G [02:13<02:10, 17.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.76G/5.00G [02:13<02:12, 16.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.77G/5.00G [02:14<02:02, 18.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.78G/5.00G [02:15<02:07, 17.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.79G/5.00G [02:15<02:09, 17.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.80G/5.00G [02:16<01:59, 18.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.81G/5.00G [02:16<02:04, 17.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.82G/5.00G [02:17<02:07, 17.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.83G/5.00G [02:17<01:57, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.84G/5.00G [02:18<02:01, 17.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.85G/5.00G [02:19<01:53, 18.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.86G/5.00G [02:19<01:57, 18.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.87G/5.00G [02:20<01:50, 19.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.88G/5.00G [02:20<01:44, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [02:21<01:39, 21.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.90G/5.00G [02:21<01:47, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.92G/5.00G [02:22<01:41, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.93G/5.00G [02:22<01:37, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.94G/5.00G [02:23<01:34, 21.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.95G/5.00G [02:23<01:32, 22.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.96G/5.00G [02:23<01:30, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.97G/5.00G [02:24<01:28, 22.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 2.98G/5.00G [02:24<01:27, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 2.99G/5.00G [02:25<01:26, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.00G/5.00G [02:25<01:25, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.01G/5.00G [02:26<01:25, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [02:26<01:24, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.03G/5.00G [02:27<01:23, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.04G/5.00G [02:27<01:23, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.05G/5.00G [02:27<01:22, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.06G/5.00G [02:28<01:22, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.07G/5.00G [02:28<01:21, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.08G/5.00G [02:29<01:21, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.09G/5.00G [02:29<01:20, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.10G/5.00G [02:30<01:20, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.11G/5.00G [02:30<01:19, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.12G/5.00G [02:31<01:19, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.14G/5.00G [02:31<01:19, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.15G/5.00G [02:31<01:18, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.16G/5.00G [02:32<01:18, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.17G/5.00G [02:32<01:17, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.18G/5.00G [02:33<01:17, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.19G/5.00G [02:33<01:17, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.20G/5.00G [02:34<01:16, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.21G/5.00G [02:34<01:16, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.22G/5.00G [02:35<01:15, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.23G/5.00G [02:35<01:15, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.24G/5.00G [02:35<01:14, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.25G/5.00G [02:36<01:14, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.26G/5.00G [02:36<01:13, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.27G/5.00G [02:37<01:13, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.28G/5.00G [02:37<01:12, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.29G/5.00G [02:38<01:12, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.30G/5.00G [02:38<01:11, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.31G/5.00G [02:39<01:11, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.32G/5.00G [02:39<01:11, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.33G/5.00G [02:39<01:10, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.34G/5.00G [02:40<01:10, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.36G/5.00G [02:40<01:09, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.37G/5.00G [02:41<01:09, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.38G/5.00G [02:41<01:08, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.39G/5.00G [02:42<01:17, 20.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.40G/5.00G [02:43<01:24, 19.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.41G/5.00G [02:43<01:19, 20.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.42G/5.00G [02:44<01:25, 18.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.43G/5.00G [02:44<01:19, 19.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.44G/5.00G [02:45<01:24, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.45G/5.00G [02:45<01:18, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.46G/5.00G [02:46<01:14, 20.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.47G/5.00G [02:46<01:20, 19.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.48G/5.00G [02:47<01:15, 20.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.49G/5.00G [02:47<01:11, 21.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.50G/5.00G [02:48<01:09, 21.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.51G/5.00G [02:48<01:16, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.52G/5.00G [02:49<01:21, 18.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.53G/5.00G [02:50<01:24, 17.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.54G/5.00G [02:50<01:26, 16.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.55G/5.00G [02:51<01:37, 14.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.57G/5.00G [02:52<01:35, 15.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.58G/5.00G [02:53<01:42, 14.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.59G/5.00G [02:53<01:38, 14.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.60G/5.00G [02:54<01:43, 13.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.61G/5.00G [02:55<01:39, 14.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.62G/5.00G [02:56<01:51, 12.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.63G/5.00G [02:57<01:53, 12.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.64G/5.00G [02:58<02:10, 10.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.65G/5.00G [03:00<02:22, 9.49MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.66G/5.00G [03:01<02:39, 8.40MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.67G/5.00G [03:03<02:49, 7.82MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.68G/5.00G [03:04<02:55, 7.51MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.69G/5.00G [03:06<02:59, 7.29MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.70G/5.00G [03:07<02:55, 7.38MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.71G/5.00G [03:09<02:58, 7.23MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.72G/5.00G [03:10<02:54, 7.34MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.73G/5.00G [03:12<02:55, 7.21MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.74G/5.00G [03:13<02:50, 7.37MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.75G/5.00G [03:14<02:45, 7.51MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.76G/5.00G [03:16<02:41, 7.66MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.77G/5.00G [03:17<02:31, 8.09MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.79G/5.00G [03:18<02:23, 8.48MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.80G/5.00G [03:19<02:17, 8.75MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.81G/5.00G [03:20<02:13, 8.94MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.82G/5.00G [03:21<02:10, 9.05MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.83G/5.00G [03:22<02:07, 9.19MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.84G/5.00G [03:23<01:59, 9.70MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.85G/5.00G [03:24<01:58, 9.76MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.86G/5.00G [03:25<01:51, 10.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.87G/5.00G [03:26<01:51, 10.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.88G/5.00G [03:27<01:46, 10.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.89G/5.00G [03:28<01:42, 10.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.90G/5.00G [03:29<01:39, 11.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.91G/5.00G [03:30<01:37, 11.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.92G/5.00G [03:31<01:39, 10.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.93G/5.00G [03:32<01:36, 11.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.94G/5.00G [03:33<01:34, 11.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.95G/5.00G [03:34<01:32, 11.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.96G/5.00G [03:35<01:30, 11.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.97G/5.00G [03:36<01:30, 11.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 3.98G/5.00G [03:37<01:30, 11.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.00G/5.00G [03:37<01:25, 11.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.01G/5.00G [03:38<01:23, 11.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.02G/5.00G [03:39<01:22, 11.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.03G/5.00G [03:40<01:16, 12.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.04G/5.00G [03:41<01:17, 12.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.05G/5.00G [03:42<01:23, 11.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.06G/5.00G [03:43<01:21, 11.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.07G/5.00G [03:43<01:20, 11.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.08G/5.00G [03:44<01:18, 11.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.09G/5.00G [03:45<01:12, 12.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.10G/5.00G [03:46<01:12, 12.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.11G/5.00G [03:47<01:11, 12.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.12G/5.00G [03:47<01:07, 13.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.13G/5.00G [03:48<01:03, 13.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.14G/5.00G [03:49<01:05, 13.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.15G/5.00G [03:50<01:01, 13.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.16G/5.00G [03:51<01:03, 13.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.17G/5.00G [03:51<00:59, 13.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.18G/5.00G [03:52<00:57, 14.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.19G/5.00G [03:53<00:59, 13.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.20G/5.00G [03:54<00:56, 14.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.22G/5.00G [03:54<00:57, 13.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.23G/5.00G [03:55<00:55, 13.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.24G/5.00G [03:56<00:53, 14.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.25G/5.00G [03:57<00:55, 13.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.26G/5.00G [03:57<00:52, 14.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.27G/5.00G [03:58<00:50, 14.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.28G/5.00G [03:59<00:51, 14.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.29G/5.00G [03:59<00:50, 14.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.30G/5.00G [04:00<00:48, 14.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.31G/5.00G [04:01<00:46, 14.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.32G/5.00G [04:01<00:44, 15.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.33G/5.00G [04:02<00:43, 15.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.34G/5.00G [04:03<00:42, 15.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.35G/5.00G [04:03<00:38, 16.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.36G/5.00G [04:04<00:38, 16.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.37G/5.00G [04:05<00:37, 16.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.38G/5.00G [04:05<00:34, 17.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.39G/5.00G [04:06<00:31, 19.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.40G/5.00G [04:06<00:32, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.41G/5.00G [04:07<00:30, 19.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.42G/5.00G [04:07<00:28, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.44G/5.00G [04:08<00:26, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.45G/5.00G [04:08<00:25, 21.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.46G/5.00G [04:08<00:24, 22.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.47G/5.00G [04:09<00:23, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.48G/5.00G [04:09<00:22, 22.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.49G/5.00G [04:10<00:22, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.50G/5.00G [04:10<00:21, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.51G/5.00G [04:11<00:21, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.52G/5.00G [04:11<00:20, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.53G/5.00G [04:12<00:20, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.54G/5.00G [04:12<00:19, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.55G/5.00G [04:12<00:19, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.56G/5.00G [04:13<00:18, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.57G/5.00G [04:13<00:18, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.58G/5.00G [04:14<00:17, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.59G/5.00G [04:14<00:17, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.60G/5.00G [04:15<00:16, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.61G/5.00G [04:15<00:16, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.62G/5.00G [04:16<00:15, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.63G/5.00G [04:16<00:15, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.65G/5.00G [04:16<00:14, 23.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.66G/5.00G [04:17<00:14, 23.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.67G/5.00G [04:17<00:14, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.68G/5.00G [04:18<00:13, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.69G/5.00G [04:18<00:13, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.70G/5.00G [04:19<00:12, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.71G/5.00G [04:19<00:12, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.72G/5.00G [04:20<00:12, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.73G/5.00G [04:20<00:11, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.74G/5.00G [04:21<00:12, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.75G/5.00G [04:21<00:10, 22.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.76G/5.00G [04:21<00:10, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.77G/5.00G [04:22<00:09, 23.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.78G/5.00G [04:22<00:09, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.79G/5.00G [04:23<00:08, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [04:23<00:08, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.81G/5.00G [04:24<00:07, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.82G/5.00G [04:24<00:07, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.83G/5.00G [04:25<00:07, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.84G/5.00G [04:25<00:06, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.85G/5.00G [04:25<00:06, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [04:26<00:05, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.88G/5.00G [04:26<00:05, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.89G/5.00G [04:27<00:04, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.90G/5.00G [04:27<00:04, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.91G/5.00G [04:28<00:03, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.92G/5.00G [04:28<00:03, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.93G/5.00G [04:29<00:03, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.94G/5.00G [04:29<00:03, 19.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.95G/5.00G [04:30<00:02, 18.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.96G/5.00G [04:31<00:02, 15.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.97G/5.00G [04:32<00:01, 15.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 4.98G/5.00G [04:32<00:01, 14.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 4.99G/5.00G [04:33<00:00, 14.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [04:34<00:00, 18.2MB/s]\n",
            "Downloading shards:  50% 2/4 [08:30<08:37, 258.74s/it]\n",
            "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 31.5M/4.92G [00:00<00:18, 263MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 62.9M/4.92G [00:00<00:20, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 94.4M/4.92G [00:00<00:21, 223MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 126M/4.92G [00:00<00:20, 233MB/s] \u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 157M/4.92G [00:00<00:21, 223MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 189M/4.92G [00:00<00:20, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 220M/4.92G [00:00<00:19, 236MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 252M/4.92G [00:01<00:18, 254MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 283M/4.92G [00:01<00:19, 239MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 315M/4.92G [00:01<00:18, 246MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 346M/4.92G [00:01<00:18, 249MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 377M/4.92G [00:01<00:17, 254MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 409M/4.92G [00:01<00:18, 246MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 440M/4.92G [00:01<00:17, 261MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 472M/4.92G [00:01<00:17, 254MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 503M/4.92G [00:02<00:20, 216MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 535M/4.92G [00:02<00:18, 232MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 566M/4.92G [00:02<00:18, 239MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 598M/4.92G [00:02<00:19, 222MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 629M/4.92G [00:02<00:18, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 661M/4.92G [00:02<00:20, 211MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 692M/4.92G [00:02<00:19, 216MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 724M/4.92G [00:03<00:18, 230MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 765M/4.92G [00:03<00:17, 232MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 797M/4.92G [00:03<00:16, 248MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 828M/4.92G [00:03<00:19, 214MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 860M/4.92G [00:03<00:18, 225MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 891M/4.92G [00:04<00:25, 160MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 923M/4.92G [00:04<00:21, 184MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 954M/4.92G [00:04<00:19, 207MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 986M/4.92G [00:04<00:18, 208MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.02G/4.92G [00:04<00:20, 190MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.05G/4.92G [00:04<00:19, 194MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.08G/4.92G [00:04<00:18, 202MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.11G/4.92G [00:05<00:39, 95.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.13G/4.92G [00:09<02:47, 22.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.17G/4.92G [00:09<01:44, 35.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.21G/4.92G [00:09<01:17, 48.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.24G/4.92G [00:09<00:57, 63.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.27G/4.92G [00:09<00:44, 81.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.31G/4.92G [00:09<00:32, 112MB/s] \u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.34G/4.92G [00:09<00:26, 135MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.38G/4.92G [00:09<00:20, 169MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.42G/4.92G [00:09<00:18, 190MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.45G/4.92G [00:10<00:16, 209MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.49G/4.92G [00:10<00:14, 232MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.53G/4.92G [00:10<00:12, 269MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.57G/4.92G [00:10<00:13, 251MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.61G/4.92G [00:10<00:11, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.66G/4.92G [00:10<00:10, 305MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.70G/4.92G [00:10<00:13, 243MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.74G/4.92G [00:11<00:11, 272MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.77G/4.92G [00:11<00:11, 266MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.81G/4.92G [00:11<00:10, 287MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.85G/4.92G [00:11<00:10, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.88G/4.92G [00:11<00:11, 258MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.92G/4.92G [00:11<00:10, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [00:11<00:10, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 1.99G/4.92G [00:12<00:11, 263MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.03G/4.92G [00:12<00:10, 265MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.07G/4.92G [00:12<00:11, 254MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.10G/4.92G [00:12<00:10, 260MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.13G/4.92G [00:12<00:11, 245MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.17G/4.92G [00:12<00:10, 250MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.20G/4.92G [00:12<00:10, 251MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.24G/4.92G [00:12<00:09, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.28G/4.92G [00:13<00:11, 229MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.31G/4.92G [00:13<00:10, 244MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.35G/4.92G [00:13<00:09, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.38G/4.92G [00:13<00:13, 189MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.41G/4.92G [00:13<00:13, 188MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.45G/4.92G [00:14<00:11, 222MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.50G/4.92G [00:14<00:09, 247MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.53G/4.92G [00:14<00:09, 250MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.57G/4.92G [00:14<00:08, 264MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.61G/4.92G [00:14<00:08, 288MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.64G/4.92G [00:14<00:08, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.67G/4.92G [00:14<00:08, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.72G/4.92G [00:14<00:07, 306MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.76G/4.92G [00:15<00:08, 246MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.79G/4.92G [00:15<00:08, 247MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [00:15<00:08, 249MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.85G/4.92G [00:15<00:08, 240MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.88G/4.92G [00:15<00:08, 228MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.92G/4.92G [00:15<00:08, 233MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.95G/4.92G [00:15<00:08, 231MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.98G/4.92G [00:16<00:07, 247MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 3.01G/4.92G [00:16<00:08, 215MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.04G/4.92G [00:16<00:08, 217MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.07G/4.92G [00:16<00:08, 206MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.10G/4.92G [00:16<00:08, 213MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.14G/4.92G [00:16<00:08, 220MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.17G/4.92G [00:16<00:07, 219MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.20G/4.92G [00:17<00:07, 229MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.23G/4.92G [00:17<00:07, 228MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.26G/4.92G [00:17<00:07, 221MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.29G/4.92G [00:17<00:07, 231MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.32G/4.92G [00:17<00:06, 231MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.36G/4.92G [00:17<00:06, 237MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.39G/4.92G [00:17<00:06, 245MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.42G/4.92G [00:18<00:06, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.45G/4.92G [00:18<00:06, 237MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.48G/4.92G [00:18<00:06, 224MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.51G/4.92G [00:18<00:06, 219MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.54G/4.92G [00:18<00:05, 232MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.58G/4.92G [00:18<00:05, 235MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.61G/4.92G [00:18<00:05, 228MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.64G/4.92G [00:18<00:05, 228MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.67G/4.92G [00:19<00:05, 241MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.70G/4.92G [00:19<00:04, 258MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.73G/4.92G [00:19<00:06, 173MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.76G/4.92G [00:19<00:06, 192MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.80G/4.92G [00:19<00:05, 214MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.83G/4.92G [00:19<00:04, 229MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.86G/4.92G [00:19<00:04, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.89G/4.92G [00:20<00:04, 206MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.92G/4.92G [00:20<00:04, 222MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.95G/4.92G [00:20<00:04, 230MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.98G/4.92G [00:20<00:03, 234MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.02G/4.92G [00:20<00:03, 237MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.05G/4.92G [00:20<00:03, 241MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.08G/4.92G [00:20<00:03, 243MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.11G/4.92G [00:21<00:03, 239MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.14G/4.92G [00:21<00:03, 253MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.17G/4.92G [00:21<00:03, 242MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.22G/4.92G [00:21<00:02, 256MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.25G/4.92G [00:21<00:02, 254MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.29G/4.92G [00:21<00:02, 285MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.32G/4.92G [00:21<00:02, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.36G/4.92G [00:21<00:01, 296MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.39G/4.92G [00:22<00:01, 301MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.42G/4.92G [00:22<00:01, 299MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.46G/4.92G [00:22<00:01, 303MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.49G/4.92G [00:22<00:01, 290MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.52G/4.92G [00:22<00:01, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.55G/4.92G [00:22<00:01, 266MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.59G/4.92G [00:22<00:01, 291MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.63G/4.92G [00:22<00:00, 306MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.68G/4.92G [00:22<00:00, 320MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.72G/4.92G [00:23<00:00, 299MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.76G/4.92G [00:23<00:00, 325MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.80G/4.92G [00:25<00:02, 55.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.85G/4.92G [00:25<00:00, 81.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:25<00:00, 191MB/s]\n",
            "Downloading shards:  75% 3/4 [08:56<02:32, 152.47s/it]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   4% 41.9M/1.17G [00:00<00:03, 346MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   8% 94.4M/1.17G [00:00<00:02, 396MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  12% 136M/1.17G [00:00<00:02, 361MB/s] \u001b[A\n",
            "model-00004-of-00004.safetensors:  15% 178M/1.17G [00:00<00:03, 324MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  19% 220M/1.17G [00:03<00:27, 34.8MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  23% 273M/1.17G [00:03<00:16, 53.8MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  27% 315M/1.17G [00:03<00:11, 73.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  31% 357M/1.17G [00:04<00:08, 93.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  33% 388M/1.17G [00:04<00:06, 112MB/s] \u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 430M/1.17G [00:04<00:05, 141MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  39% 461M/1.17G [00:04<00:04, 164MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 503M/1.17G [00:04<00:03, 196MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  47% 545M/1.17G [00:04<00:02, 224MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  50% 587M/1.17G [00:04<00:02, 251MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 629M/1.17G [00:04<00:02, 260MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 671M/1.17G [00:05<00:01, 282MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  61% 713M/1.17G [00:05<00:01, 295MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  65% 755M/1.17G [00:05<00:01, 280MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  67% 786M/1.17G [00:05<00:01, 275MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 818M/1.17G [00:05<00:01, 257MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  73% 849M/1.17G [00:05<00:01, 253MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 881M/1.17G [00:05<00:01, 239MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 912M/1.17G [00:06<00:01, 248MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  81% 944M/1.17G [00:06<00:00, 234MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 975M/1.17G [00:06<00:00, 244MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  86% 1.01G/1.17G [00:06<00:00, 235MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 1.04G/1.17G [00:06<00:00, 226MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 1.07G/1.17G [00:06<00:00, 232MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 1.10G/1.17G [00:06<00:00, 234MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 1.13G/1.17G [00:07<00:00, 233MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:07<00:00, 162MB/s]\n",
            "Downloading shards: 100% 4/4 [09:04<00:00, 136.01s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-08-21 03:21:49,399 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-08-21 03:21:49,401 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [01:28<00:00, 22.04s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-08-21 03:23:18,730 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-08-21 03:23:18,730 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at shenzhi-wang/Llama3-8B-Chinese-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 943kB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-08-21 03:23:19,256 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-08-21 03:23:19,256 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128009\n",
            "}\n",
            "\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,gate_proj,k_proj,up_proj,v_proj,down_proj,o_proj\n",
            "08/21/2024 03:26:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:642] 2024-08-21 03:26:33,606 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-08-21 03:26:34,100 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-08-21 03:26:34,100 >>   Num examples = 78\n",
            "[INFO|trainer.py:2130] 2024-08-21 03:26:34,100 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-08-21 03:26:34,100 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2134] 2024-08-21 03:26:34,100 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2135] 2024-08-21 03:26:34,100 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-08-21 03:26:34,100 >>   Total optimization steps = 12\n",
            "[INFO|trainer.py:2137] 2024-08-21 03:26:34,105 >>   Number of trainable parameters = 20,971,520\n",
            " 42% 5/12 [32:33<45:28, 389.79s/it]08/21/2024 03:59:07 - INFO - llamafactory.train.callbacks - {'loss': 3.3390, 'learning_rate': 3.1470e-05, 'epoch': 1.03, 'throughput': 41.44}\n",
            "{'loss': 3.339, 'grad_norm': 1.235205888748169, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.03, 'num_input_tokens_seen': 80944}\n",
            " 83% 10/12 [1:05:06<13:02, 391.45s/it]08/21/2024 04:31:40 - INFO - llamafactory.train.callbacks - {'loss': 3.1878, 'learning_rate': 3.3494e-06, 'epoch': 2.05, 'throughput': 41.49}\n",
            "{'loss': 3.1878, 'grad_norm': 0.9404476284980774, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.05, 'num_input_tokens_seen': 162096}\n",
            "100% 12/12 [1:17:54<00:00, 387.40s/it][INFO|trainer.py:3478] 2024-08-21 04:44:29,039 >> Saving model checkpoint to saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/checkpoint-12\n",
            "[INFO|configuration_utils.py:733] 2024-08-21 04:44:29,664 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-21 04:44:29,665 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-21 04:44:29,895 >> tokenizer config file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/checkpoint-12/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-21 04:44:29,896 >> Special tokens file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/checkpoint-12/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-08-21 04:44:34,348 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4680.2424, 'train_samples_per_second': 0.05, 'train_steps_per_second': 0.003, 'train_loss': 3.242769638697306, 'epoch': 2.46, 'num_input_tokens_seen': 194016}\n",
            "100% 12/12 [1:18:00<00:00, 390.02s/it]\n",
            "[INFO|trainer.py:3478] 2024-08-21 04:44:34,350 >> Saving model checkpoint to saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30\n",
            "[INFO|configuration_utils.py:733] 2024-08-21 04:44:34,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-21 04:44:34,907 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-21 04:44:35,178 >> tokenizer config file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-21 04:44:35,179 >> Special tokens file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.4615\n",
            "  num_input_tokens_seen    =     194016\n",
            "  total_flos               =  8159191GF\n",
            "  train_loss               =     3.2428\n",
            "  train_runtime            = 1:18:00.24\n",
            "  train_samples_per_second =       0.05\n",
            "  train_steps_per_second   =      0.003\n",
            "Figure saved at: saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30/training_loss.png\n",
            "08/21/2024 04:44:35 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "08/21/2024 04:44:35 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2024-08-21 04:44:35,558 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 05:10:50,280 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 05:10:50,280 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 05:10:50,280 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-21 05:10:50,280 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-08-21 05:10:50,723 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "08/21/2024 05:10:50 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-08-21 05:10:50,957 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-21 05:10:50,959 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"shenzhi-wang/Llama3-8B-Chinese-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "08/21/2024 05:10:50 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "08/21/2024 05:10:50 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-08-21 05:10:51,100 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-08-21 05:10:51,103 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-08-21 05:10:51,105 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [01:41<00:00, 25.42s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-08-21 05:12:33,501 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-08-21 05:12:33,501 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at shenzhi-wang/Llama3-8B-Chinese-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-08-21 05:12:33,773 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-08-21 05:12:33,773 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128009\n",
            "}\n",
            "\n",
            "08/21/2024 05:16:07 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "08/21/2024 05:16:08 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-21-03-11-30\n",
            "08/21/2024 05:16:08 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n",
            "08/21/2024 05:16:08 - WARNING - llamafactory.chat.hf_engine - There is no current event loop, creating a new one.\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 2 開啟WebUI\n",
        "#@markdown #STEP 2\n",
        "#@markdown ##開啟WebUI\n",
        "#@markdown ##Run the WebUI\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title STEP 2-1 使用指令進行訓練\n",
        "# @markdown #STEP 2-1\n",
        "# @markdown  ## 此步驟是非必要，看個人使用習慣\n",
        "! CUDA_VISIBLE_DEVICES=0 python src/train.py \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path shenzhi-wang/Llama3-8B-Chinese-Chat \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template llama3 \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset myDataset \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 2e-05 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 100 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-08-20-05-04-08 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --quantization_bit 4 \\\n",
        "    --quantization_method bitsandbytes \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.01 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 100 \\\n",
        "    --per_device_eval_batch_size 1"
      ],
      "metadata": {
        "id": "_CWt_vykicuq",
        "outputId": "550e878d-c270-4d39-b044-dfb30879e517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/src/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I11rZlhctzd9"
      },
      "source": [
        "#STEP 3 上傳輸出的檔案到 HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rte6fcaejOsC"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.1 安裝HuggingFace API\n",
        "#@markdown #STEP 3.1\n",
        "#@markdown ##為了上傳模型至HuggingFace，需要先安裝HF的函式庫\n",
        "#@markdown ##In order to upload the model to HuggingFace, we have to install the HF library first.\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRbS8HBWTZpu"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.2 使用API Token登入\n",
        "#@markdown #STEP 3.2\n",
        "#@markdown ##執行本儲存格後填入 API Token(需要有 Write 權限)，然後按下登入\n",
        "#@markdown ##After runing this cell, fill in the API Token (Write permission is required), and then click Login\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd7sv-ZvT4KB"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.3 上傳模型\n",
        "#@markdown #STEP 3.3\n",
        "#@markdown ##執行本儲存格後填入 API Token(需要有 Write 權限)，然後按下登入\n",
        "#@markdown ##After runing this cell, fill in the API Token (Write permission is required), and then click Login\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"username/fine-tune-model\" #@param {type:\"string\"}\n",
        "export_folder_path = \"/content/LLaMA-Factory/export\" #@param {type:\"string\"}\n",
        "api.create_repo(model_id, private=True, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_folder(\n",
        "    folder_path=export_folder_path,\n",
        "    repo_id=model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7VH8k-s0G71"
      },
      "source": [
        "#STEP 4 保存到Google Drive(自選)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2StL2Wqt0MAt"
      },
      "outputs": [],
      "source": [
        "#@title STEP 4.1 連接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72nLh_L50g74"
      },
      "outputs": [],
      "source": [
        "#@title STEP 4.2 複製檔案到Google Drive\n",
        "Google_Drive_Folder = \"/content/drive/MyDrive/LLaMA-Factory/\" #@param {type:\"string\"}\n",
        "\n",
        "!cp -rf /content/LLaMA-Factory/saves {Google_Drive_Folder}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}